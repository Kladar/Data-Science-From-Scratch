{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24 - MapReduce\n",
    "\n",
    "For performing parallel processing on large datasets, but the basics are simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re, datetime\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()                       \n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   \n",
    "    return set(all_words)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First example has to involve counting words, as per the rules.\n",
    "\n",
    "# Need a function that turns a document into a sequence of key-value pairs.\n",
    "\n",
    "def wc_wrapper(document):\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)\n",
    "        \n",
    "# now we need a reducer function\n",
    "\n",
    "def wc_reducer(word, counts):\n",
    "    \"\"\"sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))\n",
    "    \n",
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "\n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized for a single machine\n",
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"runs MapReduce on the inputs using mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "\n",
    "    return [output\n",
    "            for key, values in collector.items()\n",
    "            for output in reducer(key,values)]\n",
    "            \n",
    "def reduce_with(aggregation_fn, key, values):\n",
    "    \"\"\"reduces a key-values pair by applying aggregation_fn to the values\"\"\"\n",
    "    yield (key, aggregation_fn(values))\n",
    "\n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\"turns a function (values -> output) into a reducer\"\"\"\n",
    "    return partial(reduce_with, aggregation_fn)\n",
    "\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Analyzing status updates\n",
    "status_updates = [\n",
    "    {\"id\": 1,\n",
    "     \"username\" : \"joelgrus\",\n",
    "     \"text\" : \"Is anyone interested in a data science book?\",\n",
    "     \"created_at\" : datetime.datetime(2013, 12, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"data_guy\", \"data_gal\", \"bill\"] },]\n",
    "\n",
    "# what day of the week do people talk about it the most?\n",
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)\n",
    "\n",
    "data_science_days = map_reduce(status_updates,\n",
    "                               data_science_day_mapper,\n",
    "                               sum_reducer)\n",
    "print(data_science_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', ('is', 1))]\n"
     ]
    }
   ],
   "source": [
    "# What words are used the most?\n",
    "def words_per_user_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user, (word, 1))\n",
    "\n",
    "def most_popular_word_reducer(user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count\"\"\"\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    yield (user, (word, count))\n",
    "\n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_reducer)\n",
    "\n",
    "print(user_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', 3)]\n"
     ]
    }
   ],
   "source": [
    "# number of distinct status-likers for each user?\n",
    "\n",
    "def liker_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield (user, liker)\n",
    "\n",
    "distinct_likers_per_user = map_reduce(status_updates,\n",
    "                                      liker_mapper,\n",
    "                                      count_distinct_reducer)\n",
    "print(distinct_likers_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalizing to matrix multiplication\n",
    "def matrix_multiply_mapper(m, element):\n",
    "    \"\"\"m is the common dimension (columns of A, rows of B)\n",
    "    element is a tuple (matrix_name, i, j, value)\"\"\"\n",
    "    matrix, i, j, value = element\n",
    "\n",
    "    if matrix == \"A\":\n",
    "        for column in range(m):\n",
    "            # A_ij is the jth entry in the sum for each C_i_column\n",
    "            yield((i, column), (j, value))\n",
    "    else:\n",
    "        for row in range(m):\n",
    "            # B_ij is the ith entry in the sum for each C_row_j\n",
    "            yield((row, j), (i, value))\n",
    "            \n",
    "def matrix_multiply_reducer(m, key, indexed_values):\n",
    "    results_by_index = defaultdict(list)\n",
    "    for index, value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "\n",
    "    # sum up all the products of the positions with two results\n",
    "    sum_product = sum(results[0] * results[1]\n",
    "                      for results in results_by_index.values()\n",
    "                      if len(results) == 2)\n",
    "\n",
    "    if sum_product != 0.0:\n",
    "        yield (key, sum_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map-reduce matrix multiplication\n",
      "entries: [('A', 0, 0, 3), ('A', 0, 1, 2), ('B', 0, 0, 4), ('B', 0, 1, -1), ('B', 1, 0, 10)]\n",
      "result: [((0, 0), 32), ((0, 1), -3)]\n"
     ]
    }
   ],
   "source": [
    "entries = [(\"A\", 0, 0, 3), (\"A\", 0, 1,  2),\n",
    "           (\"B\", 0, 0, 4), (\"B\", 0, 1, -1), (\"B\", 1, 0, 10)]\n",
    "mapper = partial(matrix_multiply_mapper, 3)\n",
    "reducer = partial(matrix_multiply_reducer, 3)\n",
    "\n",
    "print(\"map-reduce matrix multiplication\")\n",
    "print(\"entries:\", entries)\n",
    "print(\"result:\", map_reduce(entries, mapper, reducer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes chapter 24\n",
    "\n",
    "Most widely used mapreduce system is Hadoop, which has books written about it. You have to set up a cluster, which I hear is difficult and commonly written in Java, although there is Hadoop streaming which will let you write it in python.\n",
    "\n",
    "Amazon has elstic mapreduce which is basically mapreduce as a cloud service\n",
    "\n",
    "mrjob  is a python packae for interfacing with hadoop or elastic mapreduce.\n",
    "\n",
    "Hadoop has high latency, so there are alternative frameworks for real time analytics such as Spark and Storm.\n",
    "\n",
    "And as these things move quickly the newest framework is probably newer than the book. So it goes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
