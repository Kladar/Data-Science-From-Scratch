{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17 - Decision Trees\n",
    "\n",
    "A decision tree uses a tree structure to represent a number of possible decision paths and an outcome for each path. They are very transparent and can provide a strong prediction using all types of data, but are computationally hard to create. It's very easy to overfit to your data. We focus on classification trees (rather than regression trees) and work through the ID3 algorithm for learning a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "\"How much information do we need to get from each question?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities):\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
    "\n",
    "# for data consisting of pairs of inputs and labels, we'll need to compute the class probabilities ourselves\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)\n",
    "\n",
    "# getting at  the entropy of our partitions \n",
    "\n",
    "def partition_entropy(subsets):\n",
    "    \n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets)\n",
    "\n",
    "# one drawback is that making partitions based on labels can cause overfitting. Imagine something that relies on a \n",
    "# SSN. It would separate people into categories of 1 and not generalize at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)]\n",
    "\n",
    "# from Joel's github.\n",
    "\n",
    "# Our tree consists of decision nodes (which ask a question and partition) and leaf nodes (which provide an answer)\n",
    "# created by the ID3 method. It is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If all the data have the same label, create a leaf node that predicts that label then stop\n",
    "2. If the list of attributes is empty (no more questions to ask), then create a leaf node that predicts the most common\n",
    "    label then stop.\n",
    "3. Otherwise, try partitioning the data by each of the attributes\n",
    "4. Choose the partition with the lowest partition entropy\n",
    "5. Add a decision node based on the chosen attribute\n",
    "6. Recur on each partitioned subset using the remaining attributes\n",
    "\n",
    "This is known as a greedy algorthim because it chooses the most immediately best option at each step. There are algorthims that can back propogate and improve even if a worse move is made in the beginning, but this is a good first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for partitioning\n",
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item\n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "\n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute])\n",
    "\n",
    "#function for computing entropy\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "# find the minimum-entropy partition for the whole set\n",
    "\n",
    "for key in ['level', 'lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(inputs, key))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "# lowest entropy comes from splitting on level, so we make a subtree for each possible level value.\n",
    "\n",
    "senior_inputs = [(input, label) for input, label in inputs if input['level'] == 'Senior']\n",
    "for key in ['lang','tweets','phd']:\n",
    "    print( key, partition_entropy_by(senior_inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes tweets always result in True while no tweets always results in False, so it is a zero entropy partition. Woo!\n",
    "\n",
    "#Mid candidates are all True, so they partition themselves here. Finally, Junior candidates. We end up splitting on \n",
    "# phd after which we see that PhD always results in True and PhD always results in False. See book for actual tree.\n",
    "\n",
    "# Sometimes we are missing a label or haven't seen it before, so we can replace in here. Finally, let's generalize\n",
    "# to make our nodes (True, False, or tuple(attribute, subtree_dict))\n",
    "\n",
    "def classify(tree, input):\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "    \n",
    "    attribute, subtree_dict = tree\n",
    "    subtree_key = input.get(attribute)\n",
    "    \n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None\n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    \n",
    "    return classify(subtree, input)\n",
    "\n",
    "# now just build the tree from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "    \n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "        \n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0: return False # No trues? Return false leaf\n",
    "    if num_falses == 0: return True # No Falses? Return true leaf\n",
    "    \n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_falses # return the majority leaf\n",
    "    \n",
    "    best_attribute = min(split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees \n",
    "    subtrees = { attribute_value : build_tree_id3(subset, new_candidates) \n",
    "              for attribute_value, subset in partitions.items()}\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_tree_id3(inputs)\n",
    "\n",
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                \"lang\" : \"Java\",\n",
    "                \"tweets\" : \"yes\",\n",
    "                \"phd\" : \"no\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests \n",
    "\n",
    "We can build multiple decision trees and then let thom vote on how to classify inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]\n",
    "\n",
    "# We can build \"random\" trees by training each on bootstrapped data of the inputs. This is called \"Bootstrap Aggregating\"\n",
    "# or bagging. We can also randomly choose the next attribute to split on from a subset rather than all of the remaining\n",
    "\n",
    "    if len(split_candidates) <= self.num_split_candidates:\n",
    "        sampled_split_candidates = slpit_candidates\n",
    "    else:\n",
    "        sampled_split_candidates = random.sample(split_candidates, self.num_split_candidates)\n",
    "    \n",
    "    best_attribute = min(sampled_split_candidates, key=partial(partition_entropy_by, inputs))\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes Chapter 17\n",
    "\n",
    "For further exploration, scikit-learn has decision tree models. It also has an ensemble module that includes a RandomForestClassifier as well as other ensemble methods.\n",
    "\n",
    "Wikipedia (https://en.wikipedia.org/wiki/Decision_tree) is a good place to learn more about decision tree algos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
