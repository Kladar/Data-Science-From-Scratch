{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Multiple Regression\n",
    "\n",
    "Introducing more variables. Beta in this case is not a single variable but a vector of variables, each representing the relationship of that variable to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions from other chapters. Someday I'll refactor and make importable packages out of all the old functions\n",
    "# but for now this is it\n",
    "import random\n",
    "\n",
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha\n",
    "\n",
    "def dot(v, w):\n",
    "    return sum(v_i * w_i\n",
    "              for v_i, w_i in zip(v,w))\n",
    "\n",
    "def median(v):\n",
    "    n = len(v)\n",
    "    sorted_v = sorted(v)\n",
    "    midpoint = n//2\n",
    "    \n",
    "    if n % 2 == 1:\n",
    "        return sorted_v[midpoint]\n",
    "    else:\n",
    "        lo = midpoint -1\n",
    "        hi = midpoint # I want to get to the point where I intuit whether to add one to get hi or subtract one to get lo\n",
    "                      # at 10 pm on a thursday some day\n",
    "        return (sorted_v[lo] + sorted_v[hi]) / 2\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def total_sum_of_squares(y):\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def in_random_order(data):\n",
    "    indices = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indices) #Joel writes indexes but that's not the pluralization.\n",
    "    for i in indices:\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data = zip(x,y,)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    \n",
    "    min_theta, min_value = None, float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # we want to stop when we don't see any improvement try shrinking the step size, but if we do keep going\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "        \n",
    "        if value < min_value:\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # take a gradient step\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, that)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta = [alpha, beta_1, ..., beta_k]\n",
    "# x_i = [1, x_i1, ..., x_ik]\n",
    "\n",
    "#new predict function\n",
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)\n",
    "\n",
    "x = [1, 49, 4, 0] # independent variable with constant term, num of friends, work hours per day, and boolean \"has a Phd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important assumptions of the least squares model is that each of the columns of x are linearly independent, meaning there is no way to write any parameter as a weighted sum of the others. Violations of this assumption are typically nonobvious \n",
    "\n",
    "Another is that the parameters of x are all uncorrelated with the errors. Otherwise our estimate of beta will be systematically wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to linear regression from previous chapter: minimize error function with stochastic gradient descent\n",
    "\n",
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)\n",
    "\n",
    "def squared_errors(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, beta): # hooray calculus\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta) for x_ij in x_i]\n",
    "\n",
    "# now we can find beta using SGD\n",
    "def estimate_beta(x,y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(squared_error, squared_error_gradient, x,y, beta_initial, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_minutes_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-22ef57d44d86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_beta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# output = [30.62, 0.972, -1.868, 0.911]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_minutes_good' is not defined"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "random.seed(0)\n",
    "beta = estimate_beta(x, daily_minutes_good)\n",
    "# output = [30.62, 0.972, -1.868, 0.911]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our model looks like:\n",
    "minutes = 30.63 + 0.972*friends - 1.868*work hours + 0.911*has PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth reading the chapter here because he starts to allow for interactions among the variables, and suggests handling cases for multiplying the variables, etc. This is obviously leading to CNN type deep learing stuff that will just do that on its own (account for interactions among variables, that is). He ends by asking if these coefficients matter because there are no limits to the numbers of products, logs, squares, and higher powers we could add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodness of fit\n",
    "\n",
    "def multiple_r_squared(x,y,beta):\n",
    "    sum_of_squared_errors = sum(error(x_i, y_i, beta) ** 2 for x_i, y_i in zip (x,y))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(y)\n",
    "\n",
    "# we also want to know the errors on each individual coefficient and whether they mean anything, but we are not\n",
    "# set up to do that from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: The Bootstrap\n",
    "\n",
    "Imagine a sample of n data point generated by some distribution:\n",
    "\n",
    "data = get_sample(num_points=n)\n",
    "\n",
    "We can get the median of the sample, but it varies considerably depending on the data (100 points near 100 or 100 points nearly equally distributed near 0 and near 200). Usually we can't get new samples and keep computing the median of each. What we can do instead is bootstrap (woo) new data sets by n data points with replacement from our data and then compute the medians from those synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data, stats_fn, num_samples):\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of each set:  99.94405543380051 , 100.04454803230855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[200.11259534348648,\n",
       " 200.09491321652237,\n",
       " 200.02359714875064,\n",
       " 200.08067194244424,\n",
       " 0.9524031774659693,\n",
       " 0.9524031774659693,\n",
       " 200.099964346878,\n",
       " 200.08067194244424,\n",
       " 200.02122396547387,\n",
       " 200.1015980133037,\n",
       " 200.09491321652237,\n",
       " 200.11259534348648,\n",
       " 0.9524031774659693,\n",
       " 200.09491321652237,\n",
       " 200.08067194244424,\n",
       " 200.099964346878,\n",
       " 200.1043347115292,\n",
       " 0.9980053090153341,\n",
       " 0.94750489984989,\n",
       " 100.04454803230855,\n",
       " 200.08067194244424,\n",
       " 200.02359714875064,\n",
       " 0.7502700517993686,\n",
       " 0.6758674507055095,\n",
       " 0.8081023910411597,\n",
       " 200.08067194244424,\n",
       " 200.02359714875064,\n",
       " 0.9980053090153341,\n",
       " 0.9524031774659693,\n",
       " 200.02359714875064,\n",
       " 200.17030169808345,\n",
       " 200.09491321652237,\n",
       " 0.8366818418224948,\n",
       " 200.02359714875064,\n",
       " 0.9524031774659693,\n",
       " 200.11259534348648,\n",
       " 200.099964346878,\n",
       " 200.02122396547387,\n",
       " 200.02359714875064,\n",
       " 200.099964346878,\n",
       " 0.761559089380398,\n",
       " 200.1015980133037,\n",
       " 0.7502700517993686,\n",
       " 200.02122396547387,\n",
       " 100.04454803230855,\n",
       " 100.04454803230855,\n",
       " 200.02122396547387,\n",
       " 0.9524031774659693,\n",
       " 0.94750489984989,\n",
       " 100.04454803230855,\n",
       " 0.8366818418224948,\n",
       " 0.8081023910411597,\n",
       " 0.9524031774659693,\n",
       " 200.09491321652237,\n",
       " 100.04454803230855,\n",
       " 200.02359714875064,\n",
       " 0.8366818418224948,\n",
       " 0.8006204156183219,\n",
       " 0.94750489984989,\n",
       " 200.02122396547387,\n",
       " 200.1015980133037,\n",
       " 200.02359714875064,\n",
       " 0.8081023910411597,\n",
       " 0.94750489984989,\n",
       " 100.04454803230855,\n",
       " 0.9980053090153341,\n",
       " 200.1043347115292,\n",
       " 100.04454803230855,\n",
       " 0.7502700517993686,\n",
       " 200.099964346878,\n",
       " 0.9980053090153341,\n",
       " 200.02122396547387,\n",
       " 200.09491321652237,\n",
       " 0.8021995659953194,\n",
       " 100.04454803230855,\n",
       " 200.11919106917153,\n",
       " 200.08067194244424,\n",
       " 200.1189804642904,\n",
       " 200.09491321652237,\n",
       " 200.099964346878,\n",
       " 200.08067194244424,\n",
       " 200.08067194244424,\n",
       " 0.9524031774659693,\n",
       " 200.1043347115292,\n",
       " 200.1043347115292,\n",
       " 0.761559089380398,\n",
       " 0.9980053090153341,\n",
       " 200.02122396547387,\n",
       " 0.9980053090153341,\n",
       " 200.02122396547387,\n",
       " 200.02359714875064,\n",
       " 0.8006204156183219,\n",
       " 200.02359714875064,\n",
       " 200.11259534348648,\n",
       " 0.9524031774659693,\n",
       " 200.11259534348648,\n",
       " 0.8006204156183219,\n",
       " 0.6006056225515353,\n",
       " 0.8081023910411597,\n",
       " 0.9980053090153341]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example, these data sets\n",
    "\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "far_from_100 = ([99.5 + random.random()] + [random.random() for _ in range(50)] + [200 + random.random() for _ in range(50)])\n",
    "\n",
    "print(\"Median of each set: \", median(close_to_100),\",\", median(far_from_100))\n",
    "\n",
    "bootstrap_statistic(close_to_100, median, 100)\n",
    "bootstrap_statistic(far_from_100, median, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can take the same approach to estimating the errors in our regression coefficients\n",
    "\n",
    "def estimate_sample_beta(sample):\n",
    "    x_sample, y_sample = zip(*sample) # magic unzipping trick\n",
    "    return estimate_beta(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_minutes_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-aac659d91600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbootstrap_betas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbootstrap_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimate_sample_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbootstrap_standard_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandard_deviation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbootstrap_betas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'daily_minutes_good' is not defined"
     ]
    }
   ],
   "source": [
    "# example using chapter 5 stuff\n",
    "\n",
    "random.seed(0)\n",
    "bootstrap_betas = bootstrap_statistic(zip(x, daily_minutes_good), estimate_sample_beta, 100)\n",
    "\n",
    "bootstrap_standard_errors = [standard_deviation([beta[i] for beta in bootstrap_betas]) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normal_cdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-01d2730abd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnormal_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_hat_j\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msigma_hat_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mp_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30.63\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.174\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# constant term, ~0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.972\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.079\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# num_friends, ~0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.868\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.131\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# work_hours, ~0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-01d2730abd65>\u001b[0m in \u001b[0;36mp_value\u001b[1;34m(beta_hat_j, sigma_hat_j)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_hat_j\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_hat_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta_hat_j\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnormal_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_hat_j\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msigma_hat_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnormal_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_hat_j\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msigma_hat_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normal_cdf' is not defined"
     ]
    }
   ],
   "source": [
    "# we can test each beta against the null hypothesis with a student's t distribution (n-k DOF) or since n is much larger\n",
    "# than k, a normal_cdf\n",
    "\n",
    "def p_value(beta_hat_j, sigma_hat_j):\n",
    "    if beta_hat_j > 0:\n",
    "        return 2 * (1-normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "    \n",
    "p_value(30.63, 1.174) # constant term, ~0\n",
    "p_value(0.972, 0.079) # num_friends, ~0\n",
    "p_value(-1.868, 0.131) # work_hours, ~0\n",
    "p_value(0.911, 0.990) # PhD, 0.36\n",
    "\n",
    "# if we weren't going from scratch we could compute the t-distrition and the exact standard errors\n",
    "# Most of the coefficients have very small p-values except PhD, meaning it is probably not meaningful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "If we want to actually explain phenomena, it doesn't help if we have hundreds of coefficients even if the model is really good\n",
    "\n",
    "We can add a penalty to the error term that gets larger as beta gets larger, valuing less coefficients, and minimize the combined error and penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_penalty(beta, alpha):\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x_i, y_i, beta, alpha):\n",
    "    return error(x_i, y_i, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "# which we can then plug into SGD in the usual way\n",
    "\n",
    "def ridge_penalty_gradient(beta, alpha):\n",
    "    return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def squared_error_ridge_gradient(x_i, y_i, beta, alpha):\n",
    "    return vector_add(squared_error_gradient(x_i, y_i, beta), ridge_penalty_gradient(beta, alpha))\n",
    "\n",
    "def estimate_beta_ridge(x,y,alpha):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(partial(squared_error_ridge, alpha=alpha), partial(squared_error_ridge_gradient, alpha=alpha), \n",
    "                              x, y, beta_initial, 0.001)\n",
    "\n",
    "# as we increase alpha, the goodness of fit gets worse, but the size of beta gets smaller. See book for actual values\n",
    "# the coefficient on PhD vanishes as we increase the penalty\n",
    "\n",
    "#Note: typically we want to rescale our data before using this approach. If we changed years experience to\n",
    "#centuries, it's coefficient would increase by a factor of 100 and get penalized much more even though it's the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach is to use lasso penalty regression\n",
    "\n",
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the ridge penalty shrinks coefficients overall, lasso tends to force them to be zero which is good for learing sparse models. However, it is not amenable to gradient descent which means we won't be able to solve it from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes Chapter 15\n",
    "\n",
    "For further exploration read wikipedia pages on Regression. It has a rich history. Scikit-learn has a linear_model module that provides a linear regression model similar to this one as well as many types of regularization. Statsmodels is another Python module that has linear regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
