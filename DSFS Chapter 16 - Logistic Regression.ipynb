{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 - Logistic Regression\n",
    "\n",
    "Finding paid and unpaid users. The first attempt is to use linear regression with multiple variables to find the best model.\n",
    "\n",
    "Paid account = B0 + B1 x experience + B2 x Salary + error\n",
    "(see book for graphics)\n",
    "\n",
    "There's nothing wrong with doing it this way, but it leads to some intermediate problems.\n",
    "\n",
    "1. We'd like our outputs to be ideally 0 or 1, or somewhere in between. But the outputs of the linear model can be huge positive or negative numbers, which is not clear how to interpret.\n",
    "\n",
    "2. The model assumed that the errors were uncorrelated with the columns of x. Example: it outputs very large values for peopre with lots of experience but we know the actual values must be at most 1, which means large outputs have large error, meaning our beta estimate is biased.\n",
    "\n",
    "We'd like large positives values of dot(x_i, beta) to correspond to probabilities close to 1 and negative values correspond coles to 0. We can accomplish this by applying another function to the result. In logistic regression this is the logistics function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce, partial\n",
    "import random\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "# and conveniently\n",
    "\n",
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# likelihood finding\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))\n",
    "    \n",
    "# the overall likelihood that we see the data we do with the coefficients we have is just the product of all the individual \n",
    "# likelihoods, or the sum of all the log likelihoods:\n",
    "\n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta) for x_i, y_i in zip(x ,y))\n",
    "\n",
    "# the necessary gradient to minimize against\n",
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i( x_i, y_i, beta):\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j) for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add, [logistic_log_gradient_i(x_i, y_i, beta) for x_i, y_i in zip(x,y)])\n",
    "\n",
    "# for ML\n",
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x,y)\n",
    "    train, test = split_data(data, 1 - test_pct)\n",
    "    x_train, y_train = zip(*train)\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "        next_theta = min(next_thetas, key=target_fn) # choosing the one that minimizes the error function\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if converging\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, vaue = next_theta, next_value\n",
    "            \n",
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    return lambda *args, **kwargs: [-y for y in -f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn), negate_all(gradient_fn), theta_0, tolerance)\n",
    "\n",
    "def safe(f):\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwagrs)\n",
    "        except:\n",
    "            float('inf')\n",
    "            \n",
    "    return safe_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rescaled_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d921989c02f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescaled_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.33\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# and want to maximize log likelihood on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rescaled_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set (example, doesn't run)\n",
    "\n",
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# and want to maximize log likelihood on the training data\n",
    "\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient)\n",
    "\n",
    "# pick random starting point\n",
    "beta_0 = [random.random() for _ in range(3)]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "beta_hat = maximize_batch(fn, gradient_fn, beta_0)\n",
    "\n",
    "#alternatively you can use stochastic gradient descent, which we can see in the book. Either way we get our betas (rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not as easy to interpret, as all else being equal each coefficient adds not to the output value of the linear regression (e.g. minutes spent per day) but to the input of the logistic function. We can say qualitative things based on the positive or negative impact, but otherwise we are looking for the logistic output to tell us the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-da326b6d9434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrue_positives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfalse_positives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_negatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfalse_negatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# never seen this instantiation methode before. I like it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Goodness of Fit\n",
    "# we can use the test data we held back from the model. We can assume that we predict a paid account whenever the \n",
    "# probability exceeds 0.5\n",
    "\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0 # never seen this instantiation methode before. I like it.\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    predict = logistic(dot(beta_hat, x_i))\n",
    "    \n",
    "    if y_i == 1 and predict >= 0.5:\n",
    "        true_positives += 1\n",
    "        \n",
    "    elif y_i == 1:\n",
    "        false_negatives += 1\n",
    "        \n",
    "    elif predict >= 0.5:\n",
    "        false_positives += 1\n",
    "    else:\n",
    "        true_negatives += 1\n",
    "        \n",
    "    # this is so elegant. My intuition tells me there are lots of ways to increment the predictions but damn this is a goodun\n",
    "    \n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support vector machines find the hyperplane which maximizes the distance to the nearest point in each class. Said more simply, it separates our data most efficiently into the binary categorization that we care about. Data might not be able to be perfectly separated by a single plane, but if we move the data to higher dimensional space it might be. See the book for pretty pictures of this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes Chapter 16\n",
    "\n",
    "Further exploration in scikit-learn, which has modules for linear regression and support vector machines, and libsvm, which is the support vector machine scikit-learn is using under the hood. See it's documentation for more info. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
