{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 - Naive Bayes\n",
    "\n",
    "This chapter is about starting naively to reduce bias and get the right answer from a Bayesian perspective. Worth reading the spam filter intro about Bayesian formulation and assumptions.\n",
    "\n",
    "...and underflow of floats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in old functions from previous chapters\n",
    "\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def precision(tp, fp, fn, tn):\n",
    "    return tp / (tp+fp)\n",
    "\n",
    "# recall is the fraction of the posinives identified\n",
    "def recall(tp, fp, fn, tn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)                           # remove duplicates\n",
    "\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts\n",
    "\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "def p_spam_given_word(word_prob):\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a Class classifier\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        num_spams = len([is_spam for message, is_spam in training_set if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, num_spams, num_non_spams, self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 737, (True, False): 25, (True, True): 4})\n",
      "spammiest_hams [('[ILUG-Social] Re: Important - reenactor insurance needed', False, 0.021156030237741198), ('Conversations From GDC Europe: Bill Fulton, Zeno Colaco, Harvey Smith', False, 0.03935973252681327), ('\"Free\" Elvis Costello CD a trojan horse for DRM malware', False, 0.04245442887102641), ('EFFector 15.28: Motions Filed in Morpheus Peer-to-Peer Case,', False, 0.06113067570726612), ('Attn programmers: support offered [FLOSS-Sarai Initiative]', False, 0.15546104822167903)]\n",
      "hammiest_spams [('[scoop] CEVIRI YAZILIMLARI', True, 4.6256295236250034e-08), (\"** You're -Approved-!\", True, 4.625629523627452e-08), ('Re: Instant Quote', True, 2.073925960198574e-07), ('Email Marketing', True, 9.717356917327071e-07), ('Immediate Reply Needed', True, 2.1309305436763246e-06)]\n",
      "spammiest_words [('zzzz', 0.03723404255319149, 0.00022893772893772894), ('guaranteed', 0.047872340425531915, 0.00022893772893772894), ('reps', 0.047872340425531915, 0.00022893772893772894), ('adv', 0.047872340425531915, 0.00022893772893772894), ('account', 0.05851063829787234, 0.00022893772893772894)]\n",
      "hammiest_words [('satalk', 0.005319148936170213, 0.053800366300366304), ('spambayes', 0.005319148936170213, 0.04601648351648352), ('was', 0.005319148936170213, 0.03914835164835165), ('users', 0.005319148936170213, 0.036401098901098904), ('new', 0.005319148936170213, 0.03456959706959707)]\n"
     ]
    }
   ],
   "source": [
    "# lets test it!\n",
    "path = r'C:\\Users\\klada\\PycharmProjects\\dsfs\\SpamAssassin\\*\\*'\n",
    "\n",
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "\n",
    "        with open(fn,'r',encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    return data\n",
    "              \n",
    "def train_and_test_model(path):\n",
    "    random.seed(0)\n",
    "    train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)\n",
    "\n",
    "    classified = [(subject, is_spam, classifier.classify(subject)) for subject, is_spam in test_data]\n",
    "\n",
    "#     for _, is_spam, spam_probability in classified:\n",
    "#         print(is_spam, spam_probability)\n",
    "    \n",
    "    counts = Counter((is_spam, spam_probability > 0.5) for _, is_spam, spam_probability in classified)\n",
    "\n",
    "    print(counts)\n",
    "    \n",
    "    classified.sort(key=lambda row: row[2])\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "\n",
    "    print(\"spammiest_hams\", spammiest_hams)\n",
    "    print(\"hammiest_spams\", hammiest_spams)\n",
    "\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "    spammiest_words = words[-5:]\n",
    "    hammiest_words = words[:5]\n",
    "\n",
    "    print(\"spammiest_words\", spammiest_words)\n",
    "    print(\"hammiest_words\", hammiest_words)\n",
    "    \n",
    "train_and_test_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes Chapter 13\n",
    "\n",
    "This chapter might need revisiting because I had some trouble, but it's also the end of a long coding session so we'll see how I ues it in the future. \\\n",
    "\n",
    "scikit-learn contains BernoulliNB model which is the same Naive Bayes algorithm we implemented here, as well as other variations on the model. Joel lists ways the model can be improved, which I can revisit when I ever use this stuff in the future. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
