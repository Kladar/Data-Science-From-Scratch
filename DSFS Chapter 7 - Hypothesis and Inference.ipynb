{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This chapter is all about Hypothesis testing and Inference\n",
    "The science part of \"data science\" is about testing hypotheses. Things like \"this coin is fair\" and \"data scientists prefer Python to R\". To see if things are right we must see that they are true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up an experiment with the default position being the null hypothesis, H0, and some alternative hypothesis, H1, that we like to compare it with. We use statistics to say if we can reject the null hypothesis or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to pull in some functions from the probability notebook and it's easier to just c/p them into this \n",
    "# notebook than run the whole notebook here...\n",
    "\n",
    "# PDFs and CDFs\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def uniform_pdf(x):\n",
    "    return 1 if x >= 0 and x <1 else 0\n",
    "\n",
    "def uniform_cdf(x):\n",
    "    if x < 0: return 0\n",
    "    elif x < 1: return x\n",
    "    else: return 1\n",
    "    \n",
    "def normal_pdf(x, mu=0, sigma=1):\n",
    "    sqrt_two_pi = math.sqrt(2* math.pi)\n",
    "    return (math.exp(-(x-mu) ** 2 /2 / sigma**2)/(sqrt_two_pi * sigma))\n",
    "\n",
    "def normal_cdf(x, mu=0, sigma =1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    # check if the cdf is standard\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    \n",
    "    low_z = -10.0 # very close to zero\n",
    "    hi_z = 10.0 # very close to one\n",
    "    def peter_thiel(x):\n",
    "        if x == 0:\n",
    "            return 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2\n",
    "        mid_p = normal_cdf(mid_z)\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z\n",
    "        elif mid_p > p:\n",
    "            hi_z = mid_z\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return mid_z\n",
    "\n",
    "def bernoulli_trial(p):\n",
    "    return 1 if random.random() < p else 0\n",
    "\n",
    "def binomial(n, p):\n",
    "    return sum(bernoulli_trial(p) for _ in range(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will start coding functions for finding probabilities in the binomial distribution\n",
    "\n",
    "def normal_approximation_to_binomial(n, p):\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1-p) * n)\n",
    "    return mu, sigma\n",
    "\n",
    "# The normal cdf is the probability the variable is below a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# if it's above a threshold it's not below it\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "#it's in between if it's less than hi but not less than lo\n",
    "def normal_probability_between(lo, ho, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "    \n",
    "# it's outside if it's not between    \n",
    "def normal_probability_outsude(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)\n",
    "    \n",
    "# we can also define intervals centered on the mean\n",
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    tail_prob = (1-probability) / 2\n",
    "    \n",
    "    upper_bound = normal_lower_bound(tail_prob, mu, sigma)\n",
    "    lower_bound = normal_upper_bound(tail_prob, mu, sigma)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we choose to flip a coin n = 1000, X will be distributed approx. normally with mean = 500 and standard deviatian of 15.8\n",
    "# if our hypthesis of fairness is true\n",
    "\n",
    "# Make the coin...\n",
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\n",
    "\n",
    "\n",
    "# We need to decide how willing we are to get false positives, or Type 1 errors.\n",
    "# For reasons that are lost and may ultimately be the downfall of all of academic science, this is usually set to 5%\n",
    "\n",
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "\n",
    "# Assuming p really equals 0.5, there's just a 5% chance we observe an X that lies outside this interval\n",
    "# Approx 19 out of 20 tests will give the correct result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "# We can also test Power, which is the probability of not rejecting the null hypothesis even though it's false\n",
    "# Known as a Type 2 error.\n",
    "\n",
    "#0.95 bounds based on assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "\n",
    "# actual mu and sigma based on p = 0.55 (making an unfair coin)\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9363794803307173"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can check more directional stuff. Imagine we say our null hypothesis is that the coin is not biased toward heads\n",
    "# H0 = (p<=0.5)\n",
    "# We want a one-sided test tha rejects the null hypothesis when X is much larger than 500 but not when X is smaller than 500.\n",
    "\n",
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "print(hi) # it's less than 531 since we need \"more probability\" in the upper tail. Scoop up the probabilities and move them over\n",
    "\n",
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "# Joel doesn't make this function but why not\n",
    "def power(type_2_probability):\n",
    "    return 1 - type_2_probability\n",
    "\n",
    "power(type_2_probability)\n",
    "\n",
    "# This is a more powerful test since it no longer rejects H0 when X is below 469 (unlikely if H1 is true) and instead\n",
    "# rejects H0 when X is between 526 and 531 (somewhat likely if H1 is true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This naturally leads us into thinking about P values. Instead of picking an interval, we compute the probability\n",
    "# assuming H0, that we would see the value as extreme as what we see.\n",
    "\n",
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    \n",
    "    else:\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "    \n",
    "# So if we see 530 heads, we would compute\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)\n",
    "\n",
    "# (Side note: why did we use 529.5 instead of 530? \n",
    "# Because normal_probability_between(529.5, 530.5, mu_0, sigma_0) is a better estimate than\n",
    "# normal_probability_between(530, 531, mu_0, sigma_0) so correspondingly, normal_probability_above(529.5, mu_0, sigma_0)\n",
    "# is better for at least 530 heads. In case you were wondering.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06119\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simulation\n",
    "import random\n",
    "\n",
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() <0.5 else 0 for _ in range(1000))\n",
    "    \n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        extreme_value_count += 1\n",
    "        \n",
    "print(extreme_value_count / 100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the p-value is greater than our 5% so we don't reject the null. If we saw 532 heads, the p-value would be:\n",
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582083"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is less than our p-value so we reject the null.\n",
    "# similarly we could have\n",
    "\n",
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below\n",
    "\n",
    "# for our one-sided test, if we saw 525 heads we would compute:\n",
    "upper_p_value(524.5, mu_0, sigma_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which means we wouldn't reject the null. If we sayw 527 heads\n",
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we would reject the null.\n",
    "# This of course assumes our data is normally distributed. There are various normality tests, but plotting is a good start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "If we observe 525 out of 1000 flips as heads, we estimate p =0.525. Confidence intervals are a good way to say how \n",
    "confident we are in that prediction. The central limit theorem tells us that the average of those bernoulli variable should be approximately normal, with mean p and standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "# math.sqrt(p * (1-p) * n)\n",
    "\n",
    "# we don't actually know p, so we use our estimate\n",
    "\n",
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1-p_hat)/1000)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the normal approximation, we coclude that we are 95% confident that the following interval contains the true \n",
    "# parameter p\n",
    "\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a statement about the interval, not p. It means that if we were to repeart the experiment many times, 95% of the time the \"true\" parameter (which is always the same) would lie within the observed confidence interval (which could be different every time).\n",
    "\n",
    "We do not conclude that the coin is unfair, since 0.5 wall within our confidence interval.\n",
    "Instead if we had 540 heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015760710643876435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1-p_hat)/1000)\n",
    "print(sigma)\n",
    "normal_two_sided_bounds(0.95, mu, sigma)\n",
    "\n",
    "# fair coin does not lie in the confidence interval. Left side is greater than 0.5, which is fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-hacking: The bane of science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "# 5% of the time we will erroneously reject the null hypothesis (see something we don't, chase noise, etc) if our confidence\n",
    "# is set to 5%\n",
    "\n",
    "def run_experiment():\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "num_rejections = len([experiment for experiment in experiments if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can usually find significant results if you throw enough hypothesis at you data set. Remove the outliers and you can probably get something less than 5%. This is p-hacking and sometimes a consequence of the inference from p-values framework.\n",
    "\n",
    "\"If you want to do good *science*, you should determine your hypotheses before looking at the data, you should clean your data without hypothesis in mind, and you should keep in mind that p-values are not substitutes for common sense.\n",
    "\n",
    "We'll talk about Bayesian inference later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "# Case study: Running an A/B test\n",
    "# (Joel uses the example of getting people to click on ads but that sounds horrid (he says as much) so I came up with my own)\n",
    "\n",
    "# We're A/B testing how much joy something we show people brings them: which is more upvoted?\n",
    "# We don't know the actual standard deviations so we should really be using a t_test but we are using a lot of examples\n",
    "# so the difference isn't all that important\n",
    "\n",
    "def estimated_parameters(N, n):\n",
    "    p = n/N\n",
    "    sigma = math.sqrt(p * (1-p) / N)\n",
    "    return p, sigma\n",
    "\n",
    "# We say that one gets 200 while another gets 180 out of 1000 upvotes\n",
    "\n",
    "def ab_test_stat(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    \n",
    "    return ((p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2))\n",
    "\n",
    "z = ab_test_stat(1000, 200, 1000, 180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The probability you'd see such a difference if the means were actually equal is \n",
    "\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.948839123097944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25%, which is not enough to reject\n",
    "\n",
    "# If one only got 150 upvotes on one:\n",
    "\n",
    "z = ab_test_stat(1000, 200, 1000, 150)\n",
    "print(z)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a 3% chance we'd see such a large difference if they had the same mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "So far we've just been doing confidence statements about our *tests*. The alternative approach is to treat the unknown parameters as random variables themselves. We can start with a prior distribution for the parameters and then use the observed data and Baye's Theorem to get an updated posterior distribution for the parameters. Rather than make probability judgements about the tests, we make probability judgements about the parameters themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the unknown parameter is a probability, we can take a prior from the Beta distribution, putting it between 0 and 1\n",
    "\n",
    "def B(alpha, beta):\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x, alpha, beta):\n",
    "    if x <= 0 or x >= 1:\n",
    "        return 0\n",
    "    return x ** (alpha -1) * (1-x) ** (beta - 1) / B(alpha, beta)\n",
    "\n",
    "#we might use these later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I'm just going to essentially copy what Joel writes to finish the chapter because it's worth having in here, but there's not really any more code.)\n",
    "\n",
    "If alpha and beta are both 1, it's just a uniform distribution around 0.5 very dispersed. If alpha is much larger than beta, most of the weight is near 1. And if alpha is much smaller than beta, moth of the weight is near zero.\n",
    "We set our prediction (maybe that it's fair) and flip our coin. Our posterior distribution for p is again a Beta distribution but with parameters alpha + h and beta + t. (*Note: Beta distribution is just a conjugate prior to the binomial distribution, meaning if you update the Beta dist from a binomial, you'll get back a Beta dist*)\n",
    "\n",
    "Let's say you flip 10 times and see 3 heads. You started with uniform prior so the new Beta is Beta (4,8) centered around 0.33. Since you said everything is equally likely, your best guess is something pretty coles to the observed probability. If you started with Beta(20,20) (saying it's roughly fair) you'd get Beta(23,27) around 0.46 indicating a revised belief that it might bias toward tails. If you started with Beta(30,10), meaning 75% heads bias, you'd get Beta(33,17) centered around 0.66. You still believe the heads bias, but not as much anymore. \n",
    "\n",
    "If you flip more and more, the prior would matter less and less and you'd have nearly the same posterior distribution on matter which prior you started with. No matter how biased you thought the coin was it'd be hard to maintain that after seeing 1000 heads in 2000 flips (unless your prior was ridiculous).\n",
    "\n",
    "Bayesian inference can be controversial because of the complicated background mathematics and the subjective nature of choosing a prior.\n",
    "\n",
    "We've just scratched the surface of statistical inference. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
