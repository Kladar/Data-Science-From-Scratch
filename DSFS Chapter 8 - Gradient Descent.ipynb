{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Gradient Descent\n",
    "\n",
    "This chapter deals with solving optimization problems from scratch through gradient descent.\n",
    "There's lots of helpful illustrations in this chapter so I recommend reading it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need some functions from previous chapters\n",
    "import math\n",
    "\n",
    "def vector_subtract(v, w): \n",
    "    return [v_i - w_i\n",
    "           for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_dist(v,w):\n",
    "    return sum_of_squares(vector_subtract(v,w))\n",
    "\n",
    "def distance(v,w):\n",
    "    return magnitude(vector_subtract(v,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some function f that takes in a vector of real numbers and outputs a single real number\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "# we'll need to often maximize or minimize such functions\n",
    "# the gradient is the vector of partial derivatives, df/dx, and gives the input direction in which the function most quickly increases\n",
    "# Take a small step in the direction of the gradient and re-calculate\n",
    "\n",
    "#beware of local minima, or local-maxima.com (MY PODCAST!)\n",
    "\n",
    "# we want ot measure how f(x) changes when we make a small change in x\n",
    "\n",
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "# as h approaches zero, or the limit of the difference quotient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f3fe51a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can take the actual derivative of a lot of functions and simultaneously remember the best part of high school\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "# but what if we can't explicitly find the gradient? We can find the difference quotient given very small e.\n",
    "from functools import partial \n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "\n",
    "# let's plot it to see \n",
    "import matplotlib.pyplot as plt\n",
    "x = list(range(-10, 10))\n",
    "y_1 = list(map(derivative, x)) # work around for matplotlib not taking generators as inputs. \n",
    "y_2 = list(map(derivative_estimate, x))\n",
    "\n",
    "plt.title(\"Actual Derivatives vs Estimates\")\n",
    "plt.plot(x, y_1, 'rx', label='Actual')\n",
    "\n",
    "plt.plot(x, y_2, \"b+\", label='Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0444801517282768e-06, 3.65568053104897e-06, -3.1334404551848313e-06]\n"
     ]
    }
   ],
   "source": [
    "# we'll need partial derivatives, the derivative with respect to one variable holding the others constant\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h,):\n",
    "    w = [v_j + (h if j == i else 0)]\n",
    "    \n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "           for i, _ in enumerate(v)]\n",
    "\n",
    "# this is computationally expensive. if v has length n, estimate_gradient() has to evaluate f on 2n inputs. beware...\n",
    "\n",
    "# Let's get to the minimum of sum of squares through gradient descent\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "import random\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)\n",
    "    next_v = step(v, gradient, -0.01)\n",
    "    if distance(next_v, v) < tolerance:\n",
    "        break\n",
    "    v = next_v\n",
    "print(v)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the smaller you make the tolerance, the closer you get to the actual answer which is obviously 0,0,0\n",
    "\n",
    "# choosing step size is as much art as science. We can try a bunch to estimate which is best\n",
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# we can make a safety function that makes sure certain inputs that break our function don't get through\n",
    "def safe(f):\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwagrs)\n",
    "        except:\n",
    "            float('inf')\n",
    "            \n",
    "    return safe_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put it all together\n",
    "We have same target function and it's gradient. We could be minimizing errors in a model as a function of its parameters. We choose a starting value (however we do that) and implement gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "        next_theta = min(next_thetas, key=target_fn) # choosing the one that minimizes the error function\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if converging\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, vaue = next_theta, next_value\n",
    "# in this case we take in all the data. we'll see later how to look at a point at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes we want to maximize, or minimize the negative of a function\n",
    "\n",
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    return lambda *args, **kwargs: [-y for y in -f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn), negate_all(gradient_fn), theta_0, tolerance)\n",
    "\n",
    "# we'll use this stuff throughout the rest of the book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "I've only ever used it in machine learning, it's nice to see it from scratch. \n",
    "\n",
    "The reason the batch method is slow is that we have to make a prediction and compute the gradient for the whole data set at each step. Usually the error functions are additive, which means the predictive error on the whole data set is simply the sum of the predictive errors for each data point. When this is the case, we can compute the gradient and take a step for only one point at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each cycle we want to iterate through our data in a random order\n",
    "\n",
    "def in_random_order(data):\n",
    "    indices = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indices) #Joel writes indexes but that's not the pluralization.\n",
    "    for i in indices:\n",
    "        yield data[i]\n",
    "        \n",
    "# and then we take a gradient step and quit when we stop getting improvements\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data = zip(x,y,)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    \n",
    "    min_theta, min_value = None, float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # we want to stop when we don't see any improvement try shrinking the step size, but if we do keep going\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "        \n",
    "        if value < min_value:\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # take a gradient step\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, that)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n",
    "# this will be faster than the batch version\n",
    "\n",
    "# we want a maximizer as well\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn), negate_all(gradient_fn), x, y, theta_0, alpha_0) # quite the one-liner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes Chapter 8.\n",
    "\n",
    "We'll be using these functions a lot in the rest of the book. For a calculus refresher there's Active Calculus, which does sound nice than Thomas' Calculus. Thomas' calculus is all of undergraduate calculus in one place, and actually a pretty good textbook, but it looks nicer on the shelf than it does for refreshing reading.\n",
    "\n",
    "Scikit learn and tensorflow both have stochastic gradient descent functions that might be nicer that ours (and obviously keras does everything for you) but"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
